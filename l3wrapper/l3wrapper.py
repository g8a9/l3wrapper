"""
The main module for the L3 estimator.
"""

import logging
from os.path import isdir, join, exists
from os import rename, remove
from glob import glob
import subprocess
import secrets
from l3wrapper import l3wrapper_data_path
from l3wrapper.dictionary import build_class_dict, \
                                 build_item_dictionaries, \
                                 parse_raw_rules, \
                                 write_human_readable, \
                                 build_columns_dictionary, \
                                 Transaction, \
                                 build_y_mappings
from l3wrapper.validation import check_column_names, check_dtype
from joblib import Parallel, delayed
import time
from collections import Counter
from operator import itemgetter
import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.utils.multiclass import unique_labels, check_classification_targets
import warnings
from os import chdir, getcwd, mkdir
import shutil


BIN_DIR = "bin"
TRAIN_BIN = "L3CFiltriItemTrain"
CLASSIFY_BIN = "L3CFiltriItemClassifica"
CLASSIFICATION_RESULTS = "classificati.txt"
LEVEL1_FILE = "livelloI.txt"
LEVEL2_FILE = "livelloII.txt"
LEVEL1_FILE_READABLE = "lvl1_R.txt"
LEVEL2_FILE_READABLE = "lvl2_R.txt"
FILTER_LEVEL1 = ''
FILTER_LEVEL2 = ''
FILTER_BOTH = ''


def _create_column_names(X):
    # Keep L3 1-based indexing naming
    return [f"{i}" for i in range(1, X.shape[1] + 1)]


def _dump_array_to_file(X, filestem, ext):
    with open(f"{filestem}.{ext}", "w") as fp:
        for row in X:
            row = [f"{i}" for i in row]
            fp.write(f"{','.join(row)}\n")


def _remove_fit_files(filestem):
    """Remove the files generated by the fit method.

    Retain the .cls and .diz files. These are required by the classification module of L3.
    """
    [remove(f"{filestem}{f}") for f in [
        f"_{LEVEL1_FILE}",
        f"_{LEVEL2_FILE}",
        ".cls",
        ".diz",
        "_stdout.txt",
        ".bin",
        ".data"]
    ]


def _get_matching_rules(transaction, rules, max_matching):
    if max_matching < 1:
        raise ValueError("'max_matching' must be at least 1")

    matching_rules = list()
    rule_iter = (r for r in rules if r.match(transaction))
    count = 0
    while count < max_matching:
        try:
            matching_rules.append(next(rule_iter))
            count += 1
        except StopIteration:
            break

    return matching_rules


def _get_majority_class(y):
    """Get the majority class.

    Note that if two or more classes have the same occurrences, one of them is returned arbitrarily.
    """
    from collections import Counter
    mc = Counter(y).most_common()
    return mc[0][0]


class L3Classifier(BaseEstimator, ClassifierMixin):
    """The L3-based estimator implementing the scikit-learn estimator interface.

    The model training relies on the L3 binaries. At this point they should
    be already iavailable.
    Instead, inference is enabled by the estimator itself. Hence, no L3
    binaries are used at classification time (see :meth:`predict`).

    Parameters
    ----------
    min_sup : float, default='0.01'
        The minimum support threshold to be used while training.
    min_conf : float, default='0.5'
        The minimum confidence threshold to be used while training.
    l3_root: str, default='$HOME/l3wrapper_data'
        The root folder where L3 binaries are located.
    assign_unlabeled : str, default='majority_class'
        The strategy used to assign the classification label whenever there is
        no rule that matches the data point to classified.
    match_strategy : {'majority_voting'}, default='majority_voting'
        The strategy used to pick which rule or set of rules should be used to
        classify a data point. Supported values:
        - 'majority_voting' (default): choose the label using majority voting
        among the class labels predicted by the top `max_matching` rules.
        In case of a tie, the label is chosen arbitrarily.
    max_matching : int, default=1
        The number of rules to be used for choosing the final label. It is used
        only when match_strategy='majority_voting'.
    specialistic_rules : bool, default=True
        Choose whether to prefer specialistic or general rules first at
        training time.
    max_length : int, default=0
        The maximum length of the mined rules. Note that as per the L3 training
        procedure, this applies to the *macro-itemset* mining step, i.e. it is possible
        that their relative traductions to normal itemset contain rule
        antecedents longer than max_length. (default=0, i.e. no limit)
    rule_sets_modifier : {'standard', 'level1'}, default='standard'
        Use this parameter to modify the extracted rule sets. Option
        'level1' retains only the level 1 rule set, discarding level 2.
        If 'standard', the original behavior of L3 is unchanged.

    Attributes
    ----------
    X_ : ndarray, shape (n_samples, n_features)
        The input passed during :meth:`fit`.
    y_ : ndarray, shape (n_samples,)
        The labels passed during :meth:`fit`.
    classes_ : ndarray, shape (n_classes,)
        The classes seen at :meth:`fit`.
    n_items_used_ : int
        The number of different items seen.
    lvl1_rules_ : list
        The level 1 rules (:class:`Rule`) mined at :meth:`fit`.
    lvl2_rules_ : list
        The level 2 rules (:class:`Rule`) mined at :meth:`fit`.
    n_lvl1_rules_ : int
        The number of level 1 rules.
    n_lvl2_rules_ : int
        The number of level 2 rules.
    """
    def __init__(self, min_sup=0.01, min_conf=0.5,
                 l3_root=l3wrapper_data_path,
                 assign_unlabeled='majority_class',
                 match_strategy='majority_voting',
                 max_matching=1,
                 specialistic_rules=True,
                 max_length=0,
                 rule_sets_modifier='standard'):
        self.min_sup = min_sup
        self.min_conf = min_conf
        self.l3_root = l3_root
        self.assign_unlabeled = assign_unlabeled
        self.match_strategy = match_strategy
        self.max_matching = max_matching
        self.specialistic_rules = specialistic_rules
        self.max_length = max_length
        self.rule_sets_modifier = rule_sets_modifier

    def _more_tags(self):
        return {
            'requires_fit': True,
            'allow_nan': False,
            'X_types': ['2darray', 'string']
        }

    def _get_class_label(self, matching_rules: list):
        """TODO Important method to weight majority voting"""
        class_ids = list()
        class_priority = {class_id : 0 for (class_id, _) in self._class_dict.items()}
        for rule in matching_rules:
            class_ids.append(rule.class_id)
            class_priority[rule.class_id] += rule.rule_id

        most_common = Counter(class_ids).most_common()
        most_common = sorted(most_common, key=lambda x: class_priority[x[0]])   # ascending by class priority
        most_common = sorted(most_common, key=itemgetter(1), reverse=True)      # descending by matched count

        return self._class_dict[most_common[0][0]]

    def fit(self,
            X,
            y,
            column_names=None,
            save_human_readable=False,
            remove_files=True
            ):
        """A reference implementation of a fitting function for a classifier.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The training input samples. No numerical inputs are allowed it.
        y : array-like, shape (n_samples,)
            The target values. An array of int.
        column_names : list, default=None
            A list containing the names to assign to columns in the dataset.
            They will be used when printing the human readable format of the
            rules.
        remove_files : bool, default=True
            Use this parameter to remove all the file generated by the original
            L3 implementation at training time.

        Returns
        -------
        self : object
            Returns self.
        """
        self._train_bin_path = join(self.l3_root, BIN_DIR, TRAIN_BIN)
        self._classify_bin_path = join(self.l3_root, BIN_DIR, CLASSIFY_BIN)
        self._logger = logging.getLogger(__name__)

        X = check_dtype(X)
        check_classification_targets(y)

        # Check that X and y have correct shape
        X, y = check_X_y(X, y, dtype=np.unicode_)

        # Check that y has correct values according to sklearn's policy
        unique = unique_labels(y)

        # Check that the rule sets modifier is valid
        valid_modifiers = ['standard', 'level1']
        if self.rule_sets_modifier not in valid_modifiers:
            raise NotImplementedError(
                f"The rule sets modifier specified is not" \
                f"supported. Use one of {valid_modifiers}."
            )

        # create mappings letting L3 binaries to work on strings only
        self._yorig_to_str, self._ystr_to_orig = build_y_mappings(unique)
        y = np.array([self._yorig_to_str[label] for label in y])

        # Store the classes seen during fit
        self.classes_ = [label for label in self._ystr_to_orig.keys()]

        # Define the label when no rule matches
        if self.assign_unlabeled == 'majority_class':
            self.unlabeled_class_ = _get_majority_class(y)
        else:
            self.unlabeled_class_ = self.assign_unlabeled

        self.X_ = X
        self.y_ = y

        token = secrets.token_hex(4)
        filestem = f"{token}"
        train_dir = token
        if exists(train_dir):
            raise RuntimeError(f"The training dir with token {token} already exists")
        else:
            mkdir(train_dir)
        old_dir = getcwd()
        chdir(train_dir)

        # Create column names if not provided
        if column_names is None:
            column_names = _create_column_names(X)
        check_column_names(X, column_names)
        self._column_id_to_name = build_columns_dictionary(column_names)

        # Dump X and y in a single .data (csv) file. "y" target labels are inserted as the last column
        X_todump = np.hstack([X, y.reshape(-1, 1)])
        _dump_array_to_file(X_todump, filestem, "data")

        # Invoke the training module of L3.
        if self.specialistic_rules:
            specialistic_flag = "0"
        else:
            specialistic_flag = "1"

        with open(f"{filestem}_stdout.txt", "w") as stdout:
            subprocess.run(
                [
                    self._train_bin_path,
                    filestem,                       # training file filestem
                    f"{self.min_sup * 100:.2f}",    # min sup
                    f"{self.min_conf * 100:.2f}",   # min conf
                    "nofiltro",                     # filtering measure for items (DEPRECATED)
                    "0",                            # filtering threshold (DEPRECATED)
                    specialistic_flag,              # specialistic/general rules (TO VERIFY)
                    f"{self.max_length}",           # max length allowed for rules
                    self.l3_root                    # L3 root containing the 'bin' directory with binaries
                ],
                stdout=stdout
            )

        # rename useful (lvl1) and sparse (lvl2) rule files
        rename(LEVEL1_FILE, f"{token}_{LEVEL1_FILE}")
        rename(LEVEL2_FILE, f"{token}_{LEVEL2_FILE}")

        # read the mappings of classification labels
        self._class_dict = build_class_dict(filestem)

        # read the mappings item->"column_name","value"
        self._item_id_to_item, self._item_to_item_id = build_item_dictionaries(filestem)
        self.n_items_used_ = len(self._item_id_to_item)

        # apply the rule set modifier 
        if self.rule_sets_modifier == 'level1':
            with open(f"{token}_{LEVEL2_FILE}", "w") as fp:
                self._logger.debug("Empty the level 2 rule set.")

        # parse the two rule sets and store them
        self.lvl1_rules_ = parse_raw_rules(f"{token}_{LEVEL1_FILE}")
        self.lvl2_rules_ = parse_raw_rules(f"{token}_{LEVEL2_FILE}")
        self.n_lvl1_rules_ = len(self.lvl1_rules_)
        self.n_lvl2_rules_ = len(self.lvl2_rules_)

        # translate the model to human readable format
        if save_human_readable:
            write_human_readable(f"{token}_{LEVEL1_FILE_READABLE}", self.lvl1_rules_,
                                 self._item_id_to_item, self._column_id_to_name, self._class_dict)
            write_human_readable(f"{token}_{LEVEL2_FILE_READABLE}", self.lvl2_rules_,
                                 self._item_id_to_item, self._column_id_to_name, self._class_dict)

        if remove_files:
            _remove_fit_files(token)

        chdir(old_dir)
        if remove_files and not save_human_readable:
            shutil.rmtree(train_dir)
        self.current_token_ = token # keep track of the latest token generated by the fit method

        return self

    def predict(self, X):
        """Predict the class labels for each sample in X.

        Additionally, the method helps to characterize the rules used during
        the inference. Each record to be predicted is converted into a
        Transaction and the list of transactions is saved.
        From the transactions one can retrieve:
            - which level was used to classify it (level=-1 means that no
              rule has covered the record)
            - which Rule (or rules) was used to classify it.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        y : ndarray, shape (n_samples,)
            The label for each sample.
        """
        # Check is fit had been called
        check_is_fitted(self, ['X_', 'y_'])

        # Input validation
        X = check_array(X, dtype=np.unicode_)

        self.labeled_transactions_ = list()
        y_pred = list()

        # TODO evaluate parallelization here
        for X_row in X:
            tr = Transaction(X_row, self._item_to_item_id)

            # match against level 1
            matching_rules = _get_matching_rules(tr, self.lvl1_rules_, self.max_matching)

            # if level 1 was not used, match against level 2
            if not matching_rules:
                matching_rules = _get_matching_rules(tr, self.lvl2_rules_, self.max_matching)
                if matching_rules:
                    tr.used_level = 2
            else:
                tr.used_level = 1

            if not matching_rules:
                y_pred.append(self.unlabeled_class_)
            else:
                y_pred.append(self._get_class_label(matching_rules))
                tr.matched_rules = matching_rules

            self.labeled_transactions_.append(tr)        # keep track of labeled transaction

        y_pred = [self._ystr_to_orig[label] for label in y_pred]
        return np.array(y_pred)
