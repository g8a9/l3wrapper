import logging
from os.path import isdir, join, exists
from os import rename, remove
from glob import glob
import numpy as np
import subprocess
import secrets
from l3wrapper import l3wrapper_data_path
from l3wrapper.dictionary import read_class_dict, read_item_dict, parse_raw_rules, write_human_readable
from l3wrapper.validation import check_column_names


BIN_DIR = "bin"
TRAIN_BIN = "L3CFiltriItemTrain"
CLASSIFY_BIN = "L3CFiltriItemClassifica"
CLASSIFICATION_RESULTS = "classificati.txt"
LEVEL1_FILE = "livelloI.txt"
LEVEL2_FILE = "livelloII.txt"
LEVEL1_FILE_READABLE = "lvl1_R.txt"
LEVEL2_FILE_READABLE = "lvl2_R.txt"
FILTER_LEVEL1 = ''
FILTER_LEVEL2 = ''
FILTER_BOTH = ''


def _create_column_names(X):
    return [f"{i}" for i in range(1, X.shape[1] + 1)]


def _dump_array_to_file(X, filestem, ext):
    with open(f"{filestem}.{ext}", "w") as fp:
        [fp.write(f"{','.join(row)}\n") for row in X]


def _remove_fit_files(filestem):
    """Remove the files generated by the fit method.
    
    Retain the .cls and .diz files. These are required by the classification module of L3.
    """
    [remove(f"{filestem}{f}") for f in ["_stdout.txt", ".bin", ".data"]]


def _remove_predict_files(filestem):
    pass


class L3Classifier:
    
    def __init__(self, min_sup: float, min_conf: float, l3_root: str = None):
        self.min_sup = min_sup
        self.min_conf = min_conf
        if l3_root:
            self.l3_root = l3_root
        else:
            self.l3_root = l3wrapper_data_path

        bin_dir = join(self.l3_root, BIN_DIR)
        if not exists(bin_dir) or not isdir(bin_dir):
            raise ValueError("No directory named 'bin' is present in the specified L3 root")

        self.train_bin_path = join(self.l3_root, BIN_DIR, TRAIN_BIN)
        self.classify_bin_path = join(self.l3_root, BIN_DIR, CLASSIFY_BIN)
        self.logger = logging.getLogger(__name__)
        
        self._lvl1_rules = None
        self.n_lvl1_rules = None
        self._lvl2_rules = None
        self.n_lvl2_rules = None
        self._n_fit = 0
        self._current_token = None      # keep track of the latest token generated by the fit method
        self._n_items_used = None
        self._n_classes = None

    def fit(self, X, y, column_names=None, save_human_readable=True, remove_files=True):
        """Fit the L3 model according to the given training data.
        
        1. DONE
        2. DONE
        3. (optional) post-process the mined rules
        """
        token = secrets.token_hex(4)
        filestem = f"{token}"

        # TODO the character ':' is not allowed in any column name, enforce this.
        # Create column names if not provided
        if not column_names:
            column_names = _create_column_names(X)
        check_column_names(X, column_names)

        # Dump X and y in a single .data (csv) file. "y" target labels are inserted as the last column
        X_todump = np.hstack([X, y])
        _dump_array_to_file(X_todump, filestem, "data")

        # Invoke the training module of L3.
        with open(f"{filestem}_stdout.txt", "w") as stdout:
            subprocess.run(
                [
                    self.train_bin_path,
                    filestem,                       # training file filestem
                    f"{self.min_sup * 100:.2f}",    # min sup
                    f"{self.min_conf * 100:.2f}",   # min conf
                    "nofiltro",                     # filtering measure for items (DEPRECATED)
                    "0",                            # filtering threshold (DEPRECATED)
                    "0",                            # specialistic/general rules (TO VERIFY)
                    "0",                            # max length allowed for rules (DEPRECATED)
                    self.l3_root                    # L3 root containing the 'bin' directory with binaries
                ],
                stdout=stdout
            )
        
        # rename useful (lvl1) and sparse (lvl2) rule files
        rename(LEVEL1_FILE, f"{token}_{LEVEL1_FILE}")
        rename(LEVEL2_FILE, f"{token}_{LEVEL2_FILE}")

        # read the mappings of classification labels
        class_dict = read_class_dict(filestem)

        # read the mappings item->"column_name","value"
        item_dict = read_item_dict(filestem, column_names)

        # parse the two rule sets and store them
        self._lvl1_rules = parse_raw_rules(f"{token}_{LEVEL1_FILE}")
        self._lvl2_rules = parse_raw_rules(f"{token}_{LEVEL2_FILE}")
        self.n_lvl1_rules = len(self._lvl1_rules)
        self.n_lvl2_rules = len(self._lvl2_rules)

        # translate the model to human readable format
        if save_human_readable:
            write_human_readable(f"{token}_{LEVEL1_FILE_READABLE}", self._lvl1_rules, item_dict, class_dict)
            write_human_readable(f"{token}_{LEVEL2_FILE_READABLE}", self._lvl2_rules, item_dict, class_dict)

        if remove_files:
            _remove_fit_files(token)

        self._n_classes = len(class_dict)
        self._n_items_used = len(item_dict)
        self._current_token = token
        self._n_fit += 1

    def predict(self, X, remove_files=True):
        if not self._current_token:
            raise RuntimeError("You must fit the model to data first.")
        filestem = f"{self._current_token}"

        _dump_array_to_file(X, filestem, "data")

        with open(f"{filestem}_stdout.txt", "w") as stdout:
            with open(f"{filestem}_stderr.txt", "w") as stderr:
                subprocess.run(
                    [
                        self.classify_bin_path,
                        filestem,                       # training file filestem
                        self.l3_root                    # L3 root containing the 'bin' directory with binaries
                    ],
                    stdout=stdout,
                    stderr=stderr
                )


        # with open(self.CLASSIFICATION_RESULTS, "r") as fp:
        #     for line in [x.strip() for x in fp.readlines()[2:]]:  # discard first two lines
        #         label = line.split(" ")[2]
        #         classification += [label]

        if remove_files:
            _remove_predict_files(filestem)


    def score(self, X, y):
        pass
